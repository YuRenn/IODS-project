---
title: "chapter 4"
author: "Yu Ren"
date: "2022-11-16"

output: 
  html_document:
    theme: flatly
    highlight: haddock
    toc: true
    toc_depth: 2
    number_section: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. Load the Boston data from the MASS package
```{r}
library(MASS)
library(corrplot)
library(tidyverse)
data("Boston")
```

1,1 explore the structure and the dimensions of the data
```{r}
#explore the structure
str(Boston)
```
```{r}
#explore the dimensions#
dim(Boston)
```
This dataset has 506 observations and 14 variables. And the variable crim = per capita crime rate by town; zn = proportion of residential land zoned for lots over 25,000 sq.ft; indus = proportion of non-retail business acres per town; chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise); nox = nitrogen oxides concentration (parts per 10 million); rm = average number of rooms per dwelling; dis = weighted mean of distances to five Boston employment centres; rad = index of accessibility to radial highways; tax = full-value property-tax rate per \$10,000; ptratio = pupil-teacher ratio by town; black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town; lstat = lower status of the population (percent); medv = median value of owner-occupied homes in \$1000s.


2.Show a graphical overview of the data and show summaries of the variables in the data.
```{r}
# plot matrix of the variables
pairs(Boston)
```

```{r}
# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")
```
From the correlation matrix, it is possible to look for relationships between different variables, using the variable dis as an example, the weighted average of distances to the five employment centres in Boston and the variable indus (proportion of non-retail commercial acres in each town) show a very strong negative correlation; with nox (nitrogen oxide concentration) also a strong negative correlation; and with age again a strong negative correlation. There is a relatively strong positive correlation with zn (the proportion of residential land over 25,000 square feet).

```{r}
#summaries of the variables in the data
summary(Boston)
```
3. Standardize the dataset and print out summaries of the scaled data.
```{r}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
boston_scaled
```
```{r}
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins
#Create a categorical variable of the crime rate
crime <- cut(boston_scaled$crim, 
             breaks = bins, 
             include.lowest = TRUE,
             label = c("low", "medium_low", "medium_high", "high"))
summary(crime)
```

```{r}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)
```

Here begins the splitting of the data into a training set and a test set, with a split ratio of 80% of the training set data and 20% of the test set data.
```{r}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- boston_scaled$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

summary(train)
summary(test)
nrow(train)
nrow(test)
```

4.Fit the linear discriminant analysis on the train set. Use the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables. 
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit
```

5. Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data.
```{r}
# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)
```
In this graph, the clustering of crime levels can be clearly seen, with low and medium_low crime rates concentrated in the top left and middle left; medium_high and high crime rates are concentrated in the bottom left and right. The concentration of low crime is much greater, and there is a very clear demarcation line between high crime on the right and the left.
In the red part of the graph, rad(index of accessibility to radial highway) is associated with high crime rates and zn(proportion of residential land zoned for lots over 25,000 sq.ft) is closely related to low crime rates.

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
predictions <- lda.pred$class
table(correct = correct_classes, predicted = predictions)
```


6.Calculate the distances between the observations. Run k-means algorithm on the dataset.
```{r}
library(MASS)
#reload the Boston dataset and standardize the detaset
data("Boston")
boston_scaled <- as.data.frame(scale(Boston))

#calculate the distances between the observations
dist <- dist(boston_scaled)
summary(dist)
```
k-means
```{r}
set.seed(13)
km <- kmeans(Boston, centers = 4)

# plot the Boston dataset with clusters
km <- kmeans(Boston, centers = 4)

# plot the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
pairs(Boston[1:5], col = km$cluster)
pairs(Boston[11:14], col = km$cluster)


```

```{r}
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)
```
The optimal number of clusters can be found by looking at the sum of squares within clusters, and in the graph above I think the optimal number of clusters is close to 2. At an optimal cluster number of 2, the values vary significantly.


